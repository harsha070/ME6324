{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['date', '% Iron Feed', '% Silica Feed', 'Starch Flow', 'Amina Flow',\n",
      "       'Ore Pulp Flow', 'Ore Pulp pH', 'Ore Pulp Density',\n",
      "       'Flotation Column 01 Air Flow', 'Flotation Column 02 Air Flow',\n",
      "       'Flotation Column 03 Air Flow', 'Flotation Column 04 Air Flow',\n",
      "       'Flotation Column 05 Air Flow', 'Flotation Column 06 Air Flow',\n",
      "       'Flotation Column 07 Air Flow', 'Flotation Column 01 Level',\n",
      "       'Flotation Column 02 Level', 'Flotation Column 03 Level',\n",
      "       'Flotation Column 04 Level', 'Flotation Column 05 Level',\n",
      "       'Flotation Column 06 Level', 'Flotation Column 07 Level',\n",
      "       '% Iron Concentrate', '% Silica Concentrate'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "df = pd.read_csv('dataset.csv', delimiter=',', decimal = ',')\n",
    "df = df.dropna()\n",
    "\n",
    "print(df.columns)\n",
    "\n",
    "Y = df['% Silica Concentrate']\n",
    "X = df.drop(['% Silica Concentrate', 'date'], axis = 1)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.3, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 329.70201878\n",
      "Iteration 2, loss = 3.93063378\n",
      "Iteration 3, loss = 2.14742166\n",
      "Iteration 4, loss = 1.84737680\n",
      "Iteration 5, loss = 1.65306818\n",
      "Iteration 6, loss = 1.52182198\n",
      "Iteration 7, loss = 1.44621520\n",
      "Iteration 8, loss = 1.35335402\n",
      "Iteration 9, loss = 1.30626623\n",
      "Iteration 10, loss = 1.27421509\n",
      "Iteration 11, loss = 1.27546360\n",
      "Iteration 12, loss = 1.21302317\n",
      "Iteration 13, loss = 1.19975240\n",
      "Iteration 14, loss = 1.19697464\n",
      "Iteration 15, loss = 1.16834135\n",
      "Iteration 16, loss = 1.15021631\n",
      "Iteration 17, loss = 1.16480404\n",
      "Iteration 18, loss = 1.13066806\n",
      "Iteration 19, loss = 1.15613672\n",
      "Iteration 20, loss = 1.16875977\n",
      "Iteration 21, loss = 1.06576805\n",
      "Iteration 22, loss = 1.09454916\n",
      "Iteration 23, loss = 1.11019044\n",
      "Iteration 24, loss = 1.07079995\n",
      "Iteration 25, loss = 1.11401066\n",
      "Iteration 26, loss = 1.08180774\n",
      "Iteration 27, loss = 1.06292553\n",
      "Iteration 28, loss = 1.06996109\n",
      "Iteration 29, loss = 1.05747674\n",
      "Iteration 30, loss = 1.04271485\n",
      "Iteration 31, loss = 1.06918956\n",
      "Iteration 32, loss = 1.03725571\n",
      "Iteration 33, loss = 1.05748847\n",
      "Iteration 34, loss = 1.07637536\n",
      "Iteration 35, loss = 1.03181334\n",
      "Iteration 36, loss = 1.02671406\n",
      "Iteration 37, loss = 0.98736367\n",
      "Iteration 38, loss = 1.00938715\n",
      "Iteration 39, loss = 0.99768486\n",
      "Iteration 40, loss = 0.97474960\n",
      "Iteration 41, loss = 0.98324178\n",
      "Iteration 42, loss = 0.97424709\n",
      "Iteration 43, loss = 1.01730936\n",
      "Iteration 44, loss = 0.95316258\n",
      "Iteration 45, loss = 0.96909471\n",
      "Iteration 46, loss = 0.96090428\n",
      "Iteration 47, loss = 0.94667741\n",
      "Iteration 48, loss = 0.94714349\n",
      "Iteration 49, loss = 0.95510255\n",
      "Iteration 50, loss = 0.92325020\n",
      "Iteration 51, loss = 0.93233699\n",
      "Iteration 52, loss = 0.96725394\n",
      "Iteration 53, loss = 0.92731506\n",
      "Iteration 54, loss = 0.91913913\n",
      "Iteration 55, loss = 0.91950762\n",
      "Iteration 56, loss = 0.90536027\n",
      "Iteration 57, loss = 0.91495768\n",
      "Iteration 58, loss = 0.89654497\n",
      "Iteration 59, loss = 0.90119271\n",
      "Iteration 60, loss = 0.88675995\n",
      "Iteration 61, loss = 0.87857600\n",
      "Iteration 62, loss = 0.90328154\n",
      "Iteration 63, loss = 0.86488496\n",
      "Iteration 64, loss = 0.86436730\n",
      "Iteration 65, loss = 0.86548234\n",
      "Iteration 66, loss = 0.83389664\n",
      "Iteration 67, loss = 0.84890490\n",
      "Iteration 68, loss = 0.84109553\n",
      "Iteration 69, loss = 0.84569943\n",
      "Iteration 70, loss = 0.83487882\n",
      "Iteration 71, loss = 0.83818729\n",
      "Iteration 72, loss = 0.83687114\n",
      "Iteration 73, loss = 0.78758112\n",
      "Iteration 74, loss = 0.82019849\n",
      "Iteration 75, loss = 0.80699129\n",
      "Iteration 76, loss = 0.82711192\n",
      "Iteration 77, loss = 0.78747795\n",
      "Iteration 78, loss = 0.79560123\n",
      "Iteration 79, loss = 0.79421301\n",
      "Iteration 80, loss = 0.79427193\n",
      "Iteration 81, loss = 0.77997079\n",
      "Iteration 82, loss = 0.76993276\n",
      "Iteration 83, loss = 0.77593264\n",
      "Iteration 84, loss = 0.78866974\n",
      "Iteration 85, loss = 0.77037635\n",
      "Iteration 86, loss = 0.75621037\n",
      "Iteration 87, loss = 0.77085300\n",
      "Iteration 88, loss = 0.74444337\n",
      "Iteration 89, loss = 0.73720699\n",
      "Iteration 90, loss = 0.75499473\n",
      "Iteration 91, loss = 0.74806550\n",
      "Iteration 92, loss = 0.73003268\n",
      "Iteration 93, loss = 0.71724503\n",
      "Iteration 94, loss = 0.73244522\n",
      "Iteration 95, loss = 0.73456615\n",
      "Iteration 96, loss = 0.73274779\n",
      "Iteration 97, loss = 0.70845273\n",
      "Iteration 98, loss = 0.71247874\n",
      "Iteration 99, loss = 0.72141059\n",
      "Iteration 100, loss = 0.70882009\n",
      "Iteration 101, loss = 0.69117108\n",
      "Iteration 102, loss = 0.69607757\n",
      "Iteration 103, loss = 0.70142914\n",
      "Iteration 104, loss = 0.69102811\n",
      "Iteration 105, loss = 0.71204155\n",
      "Iteration 106, loss = 0.70044631\n",
      "Iteration 107, loss = 0.68203887\n",
      "Iteration 108, loss = 0.67462422\n",
      "Iteration 109, loss = 0.67654835\n",
      "Iteration 110, loss = 0.68704444\n",
      "Iteration 111, loss = 0.67291899\n",
      "Iteration 112, loss = 0.66676609\n",
      "Iteration 113, loss = 0.67736962\n",
      "Iteration 114, loss = 0.65922644\n",
      "Iteration 115, loss = 0.65361496\n",
      "Iteration 116, loss = 0.66012181\n",
      "Iteration 117, loss = 0.66504534\n",
      "Iteration 118, loss = 0.65871120\n",
      "Iteration 119, loss = 0.64574885\n",
      "Iteration 120, loss = 0.64236501\n",
      "Iteration 121, loss = 0.64116084\n",
      "Iteration 122, loss = 0.63489991\n",
      "Iteration 123, loss = 0.64034690\n",
      "Iteration 124, loss = 0.65412219\n",
      "Iteration 125, loss = 0.62028738\n",
      "Iteration 126, loss = 0.62590680\n",
      "Iteration 127, loss = 0.62691673\n",
      "Iteration 128, loss = 0.62268441\n",
      "Iteration 129, loss = 0.61771457\n",
      "Iteration 130, loss = 0.62932107\n",
      "Iteration 131, loss = 0.61388448\n",
      "Iteration 132, loss = 0.60551255\n",
      "Iteration 133, loss = 0.60343032\n",
      "Iteration 134, loss = 0.60478192\n",
      "Iteration 135, loss = 0.60597932\n",
      "Iteration 136, loss = 0.59233281\n",
      "Iteration 137, loss = 0.58396574\n",
      "Iteration 138, loss = 0.59179478\n",
      "Iteration 139, loss = 0.59274479\n",
      "Iteration 140, loss = 0.57728043\n",
      "Iteration 141, loss = 0.58302667\n",
      "Iteration 142, loss = 0.58590228\n",
      "Iteration 143, loss = 0.57504776\n",
      "Iteration 144, loss = 0.58207048\n",
      "Iteration 145, loss = 0.56884246\n",
      "Iteration 146, loss = 0.57021947\n",
      "Iteration 147, loss = 0.56338005\n",
      "Iteration 148, loss = 0.55800232\n",
      "Iteration 149, loss = 0.55356016\n",
      "Iteration 150, loss = 0.56525990\n",
      "Iteration 151, loss = 0.54994948\n",
      "Iteration 152, loss = 0.55043647\n",
      "Iteration 153, loss = 0.55018559\n",
      "Iteration 154, loss = 0.55355429\n",
      "Iteration 155, loss = 0.54447730\n",
      "Iteration 156, loss = 0.53784849\n",
      "Iteration 157, loss = 0.53115801\n",
      "Iteration 158, loss = 0.54342240\n",
      "Iteration 159, loss = 0.52823630\n",
      "Iteration 160, loss = 0.53331952\n",
      "Iteration 161, loss = 0.52210601\n",
      "Iteration 162, loss = 0.52775231\n",
      "Iteration 163, loss = 0.51399598\n",
      "Iteration 164, loss = 0.52444421\n",
      "Iteration 165, loss = 0.51751258\n",
      "Iteration 166, loss = 0.50817567\n",
      "Iteration 167, loss = 0.49787819\n",
      "Iteration 168, loss = 0.51531661\n",
      "Iteration 169, loss = 0.49266392\n",
      "Iteration 170, loss = 0.50791931\n",
      "Iteration 171, loss = 0.51065747\n",
      "Iteration 172, loss = 0.48308150\n",
      "Iteration 173, loss = 0.49728561\n",
      "Iteration 174, loss = 0.49441234\n",
      "Iteration 175, loss = 0.48198833\n",
      "Iteration 176, loss = 0.48462467\n",
      "Iteration 177, loss = 0.49098245\n",
      "Iteration 178, loss = 0.49162763\n",
      "Iteration 179, loss = 0.46584787\n",
      "Iteration 180, loss = 0.48313014\n",
      "Iteration 181, loss = 0.47201157\n",
      "Iteration 182, loss = 0.46927205\n",
      "Iteration 183, loss = 0.47168542\n",
      "Iteration 184, loss = 0.45679920\n",
      "Iteration 185, loss = 0.45827377\n",
      "Iteration 186, loss = 0.46603788\n",
      "Iteration 187, loss = 0.45545001\n",
      "Iteration 188, loss = 0.45360653\n",
      "Iteration 189, loss = 0.45535351\n",
      "Iteration 190, loss = 0.44712996\n",
      "Iteration 191, loss = 0.46255739\n",
      "Iteration 192, loss = 0.44187406\n",
      "Iteration 193, loss = 0.44398919\n",
      "Iteration 194, loss = 0.45189763\n",
      "Iteration 195, loss = 0.44593820\n",
      "Iteration 196, loss = 0.44027639\n",
      "Iteration 197, loss = 0.43080377\n",
      "Iteration 198, loss = 0.43783212\n",
      "Iteration 199, loss = 0.44088995\n",
      "Iteration 200, loss = 0.42831025\n",
      "Iteration 201, loss = 0.42553296\n",
      "Iteration 202, loss = 0.42931418\n",
      "Iteration 203, loss = 0.42441084\n",
      "Iteration 204, loss = 0.42308170\n",
      "Iteration 205, loss = 0.41580141\n",
      "Iteration 206, loss = 0.40899198\n",
      "Iteration 207, loss = 0.41165493\n",
      "Iteration 208, loss = 0.41318580\n",
      "Iteration 209, loss = 0.40536302\n",
      "Iteration 210, loss = 0.41042936\n",
      "Iteration 211, loss = 0.40131873\n",
      "Iteration 212, loss = 0.40578566\n",
      "Iteration 213, loss = 0.40306997\n",
      "Iteration 214, loss = 0.40401321\n",
      "Iteration 215, loss = 0.39359441\n",
      "Iteration 216, loss = 0.38879106\n",
      "Iteration 217, loss = 0.39497721\n",
      "Iteration 218, loss = 0.39204715\n",
      "Iteration 219, loss = 0.39182272\n",
      "Iteration 220, loss = 0.37975720\n",
      "Iteration 221, loss = 0.37922998\n",
      "Iteration 222, loss = 0.38077607\n",
      "Iteration 223, loss = 0.37232053\n",
      "Iteration 224, loss = 0.38413128\n",
      "Iteration 225, loss = 0.36718103\n",
      "Iteration 226, loss = 0.37668099\n",
      "Iteration 227, loss = 0.37591045\n",
      "Iteration 228, loss = 0.36237561\n",
      "Iteration 229, loss = 0.36779500\n",
      "Iteration 230, loss = 0.35858175\n",
      "Iteration 231, loss = 0.35951627\n",
      "Iteration 232, loss = 0.36590999\n",
      "Iteration 233, loss = 0.36030498\n",
      "Iteration 234, loss = 0.35453768\n",
      "Iteration 235, loss = 0.34900671\n",
      "Iteration 236, loss = 0.35244529\n",
      "Iteration 237, loss = 0.35623565\n",
      "Iteration 238, loss = 0.34643619\n",
      "Iteration 239, loss = 0.34883638\n",
      "Iteration 240, loss = 0.34636229\n",
      "Iteration 241, loss = 0.34666591\n",
      "Iteration 242, loss = 0.33706767\n",
      "Iteration 243, loss = 0.34017180\n",
      "Iteration 244, loss = 0.34784894\n",
      "Iteration 245, loss = 0.32972429\n",
      "Iteration 246, loss = 0.34107691\n",
      "Iteration 247, loss = 0.32642015\n",
      "Iteration 248, loss = 0.33510937\n",
      "Iteration 249, loss = 0.33151040\n",
      "Iteration 250, loss = 0.32793019\n",
      "Iteration 251, loss = 0.32533431\n",
      "Iteration 252, loss = 0.32865617\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 253, loss = 0.33382956\n",
      "Iteration 254, loss = 0.32501602\n",
      "Iteration 255, loss = 0.32574655\n",
      "Iteration 256, loss = 0.32260398\n",
      "Iteration 257, loss = 0.32701626\n",
      "Iteration 258, loss = 0.32120319\n",
      "Iteration 259, loss = 0.31519258\n",
      "Iteration 260, loss = 0.32639590\n",
      "Iteration 261, loss = 0.31855023\n",
      "Iteration 262, loss = 0.31228370\n",
      "Iteration 263, loss = 0.31731213\n",
      "Iteration 264, loss = 0.31699999\n",
      "Iteration 265, loss = 0.31275332\n",
      "Iteration 266, loss = 0.31033864\n",
      "Iteration 267, loss = 0.30809972\n",
      "Iteration 268, loss = 0.31321343\n",
      "Iteration 269, loss = 0.31167466\n",
      "Iteration 270, loss = 0.30092634\n",
      "Iteration 271, loss = 0.30643109\n",
      "Iteration 272, loss = 0.30704785\n",
      "Iteration 273, loss = 0.30503397\n",
      "Iteration 274, loss = 0.30660433\n",
      "Iteration 275, loss = 0.30775714\n",
      "Iteration 276, loss = 0.29964046\n",
      "Iteration 277, loss = 0.30369285\n",
      "Iteration 278, loss = 0.29674261\n",
      "Iteration 279, loss = 0.30041921\n",
      "Iteration 280, loss = 0.30232985\n",
      "Iteration 281, loss = 0.30251217\n",
      "Iteration 282, loss = 0.29410018\n",
      "Iteration 283, loss = 0.29596929\n",
      "Iteration 284, loss = 0.29587665\n",
      "Iteration 285, loss = 0.29290248\n",
      "Iteration 286, loss = 0.29100579\n",
      "Iteration 287, loss = 0.28926435\n",
      "Iteration 288, loss = 0.29358120\n",
      "Iteration 289, loss = 0.29210501\n",
      "Iteration 290, loss = 0.29034461\n",
      "Iteration 291, loss = 0.28813240\n",
      "Iteration 292, loss = 0.28347956\n",
      "Iteration 293, loss = 0.28498050\n",
      "Iteration 294, loss = 0.28280193\n",
      "Iteration 295, loss = 0.28468836\n",
      "Iteration 296, loss = 0.28686189\n",
      "Iteration 297, loss = 0.28196985\n",
      "Iteration 298, loss = 0.28541636\n",
      "Iteration 299, loss = 0.28313200\n",
      "Iteration 300, loss = 0.28238172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\windo\\Miniconda3\\envs\\DL\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPRegressor(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=20, learning_rate='adaptive',\n",
       "       learning_rate_init=0.001, max_iter=300, momentum=0.9,\n",
       "       n_iter_no_change=50, nesterovs_momentum=True, power_t=0.5,\n",
       "       random_state=None, shuffle=True, solver='adam', tol=0.0001,\n",
       "       validation_fraction=0.1, verbose=True, warm_start=False)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MLPRegressor(activation = 'relu', hidden_layer_sizes=(20), learning_rate = 'adaptive', max_iter=300, n_iter_no_change = 50, verbose = True)\n",
    "model.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0541373769364353\n"
     ]
    }
   ],
   "source": [
    "Y_pred = model.predict(X_test)\n",
    "\n",
    "error = mean_squared_error(Y_test, Y_pred)\n",
    "print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test = Y_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.153555479216764\n"
     ]
    }
   ],
   "source": [
    "accuracy = 0.0\n",
    "\n",
    "n = len(Y_test)\n",
    "\n",
    "for i in range(n):\n",
    "    if(abs(Y_test[i]-Y_pred[i]) <= 0.10 * Y_test[i]):\n",
    "        accuracy = accuracy + 1\n",
    "\n",
    "print(accuracy/n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('1_10',Y_pred,delimiter=',')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
